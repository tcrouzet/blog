<article id="post-11364" itemscope itemType="https://schema.org/BlogPosting"><meta itemprop="articleSection" content="NetCulture"/><meta itemprop="name" content="Article"/><meta itemprop="inLanguage" content="fr-FR"/><meta itemprop="author" content="Thierry Crouzet"/><meta itemprop="accountablePerson" content="Thierry Crouzet"/><meta itemprop="datePublished" content="2009-10-21T12:54:37+02:00"/><meta itemprop="dateModified" content="2017-12-05T18:34:00+02:00"/><meta itemprop="url" content="/2009/10/21/l%e2%80%99information-dans-tous-ses-etats/" /><div class="poster-title poster-title-center""><div class="poster-big-spacer maxContainerWidth"></div><h1 itemprop="headline" id="title11364">L’information dans tous ses états</h1><div class="poster-subtitle"><div class="poster-serie"><a href="../../../../2009/10/19/dans-la-peau-de-finkielkraut/index.html" class="prev">&lt; </a><a href="../../../../tag/netculture">NetCulture 23</a> ● <span itemprop="datePublished" content="2009-10-21T12:54:00" title="Publié à 12:54"> <a href="../../../../2009/10">21 octobre</a> <a href="../../../../2009/index.html">2009</a></span><a href="../../../../2009/10/22/long-tail-mon-cul/index.html" class="next"> &gt;</a></div><div class="plus-contenair"><div class="sharemain"><div class="share" id="share11364"><div class="crunchify-social"><a class="icon_com icon" href="../../../../mail/index.html" target="_blank"></a><a class="icon_twitter icon" href="https://twitter.com/intent/tweet?text=L%E2%80%99information+dans+tous+ses+%C3%A9tats&amp;url=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F&amp;via=tcrouzet" target="_blank"></a><a class="icon_fb icon" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F" target="_blank"></a><a class="icon_linkedin icon" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F&amp;title=L%E2%80%99information+dans+tous+ses+%C3%A9tats" target="_blank"></a></div></div></div><div class="plus">&bull;&bull;&bull;<ul><li><a href="http://www.printfriendly.com/print?url=/2009/10/21/l%e2%80%99information-dans-tous-ses-etats/&amp;partner=sociable">Imprime le billet/PDF</a></li><li><a href="https://tcrouzet-com.translate.goog/2009/10/21/l%e2%80%99information-dans-tous-ses-etats/?_x_tr_sl=fr&_x_tr_tl=en&_x_tr_hl=en">Version EN/US</a></li><li><a href="http://tcrouzet.com/abonnement-par-mail/">Abonne-toi</a></li><li>Tags : <a href="../../../../tag/netculture/index.html">NetCulture</a> | <a href="../../../../dialogue/index.html">Dialogue</a> | <a href="../../../../series/index.html">Séries</a></li></li><li><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.fr" class="footertitre">(cc)</a>&#8239;<a href="../../../../informations/index.html">Thierry&#8239;Crouzet</a></li></ul></div></div></div></div><div class="post hentry" itemprop="articleBody" id="post-11364"><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="image1" class="image"><a href="../../../../2009/10/21/l%E2%80%99information-dans-tous-ses-etats/index.html"><img decoding="async" title="Point triple" src="https://tcrouzet.com/images_tc/2009/09/ptrible.jpg" alt="Point triple" width="450" height="346" class="paysage"></a><div class="legend">Point triple</div></div>
<p>Le vocable information a &eacute;t&eacute; employ&eacute; pour la premi&egrave;re fois comme nouvelle unit&eacute; par Ralph Hartley des Bell Labs en 1928 pour d&eacute;signer la partie utile d&rsquo;un message (en &eacute;cartant le bruit caus&eacute; par les parasites par exemple). Pour lui, l&rsquo;information mesure ce que nous apprenons (si nous avons d&eacute;j&agrave; re&ccedil;u le message, nous n&rsquo;apprenons rien).<span id="more-11364"></span></p>
<p>En 1948, Claude Shannon prolongea cette id&eacute;e et publia sa th&eacute;orie de l&rsquo;information (ou pour &ecirc;tre pr&eacute;cis sa th&eacute;orie de la communication). Pour lui, la quantit&eacute; d&rsquo;information contenue dans un message &eacute;quivaut au nombre de bits pour l&rsquo;encoder.</p>
<p>Si le message indique soit blanc, soit noir, un seul bit est n&eacute;cessaire. La mesure de l&rsquo;information selon Shannon est ind&eacute;pendante de sa signification. Un listing peut contenir beaucoup d&rsquo;informations qui ont peu de signification, un aphorisme peut contenir peu d&rsquo;information et avoir une signification profonde. La signification existe dans l&rsquo;esprit de l&rsquo;&eacute;metteur et du r&eacute;cepteur, pas dans l&rsquo;information qui circule.</p>
<p>Von Neumann n&rsquo;appr&eacute;ciait pas l&rsquo;id&eacute;e de s&eacute;parer information et signification. Il remarqua que la formule de Shannon pour d&eacute;crire l&rsquo;information &eacute;tait identique &agrave; celle pour d&rsquo;&eacute;crire l&rsquo;entropie (au signe moins pr&egrave;s). Il aurait donc mieux valu parler d&rsquo;entropie (ou de n&eacute;guentropie) mais Shannon continua &agrave; parler d&rsquo;information parce que les ing&eacute;nieurs employaient d&eacute;j&agrave; ce mot.</p>
<p>Mais qu&rsquo;est-ce que l&rsquo;entropie ? Elle indique le niveau de d&eacute;sordre dans un syst&egrave;me (alors que la n&eacute;guentropie indique son niveau d&rsquo;ordre). Le second principe de la thermodynamique stipule que l&rsquo;entropie d&rsquo;un syst&egrave;me ferm&eacute; augmente toujours jusqu&rsquo;&agrave; atteindre son maximal.</p>
<p>Exemple le plus simple. On place deux gaz diff&eacute;rents dans deux r&eacute;servoirs. Les particules sont relativement ordonn&eacute;es, les unes dans un r&eacute;servoir, les autres dans l&rsquo;autre. Si on connecte les deux r&eacute;servoirs, les particules de l&rsquo;un passent dans l&rsquo;autre jusqu&rsquo;&agrave; ce que nous obtenions un m&eacute;lange parfait, c&rsquo;est-&agrave;-dire un d&eacute;sordre maximal.</p>
<p>Mais qu&rsquo;est-ce que le d&eacute;sordre maximal ? Les physiciens ont d&eacute;couvert que c&rsquo;est l&rsquo;&eacute;tat o&ugrave; le syst&egrave;me enferme le plus d&rsquo;information possible. Exemple. Dans un livre, les lettres sont relativement ordonn&eacute;es, formant des mots qui se r&eacute;p&egrave;tent de temps &agrave; autre. Ce texte peut-&ecirc;tre compress&eacute; : on remplace chaque mot qui se r&eacute;p&egrave;te par un code plus court. En revanche, si on m&eacute;lange les lettres au hasard, on augmente le d&eacute;sordre, perd la signification mais la quantit&eacute; d&rsquo;information n&eacute;cessaire &agrave; d&eacute;crire le syst&egrave;me augmente (on ne peut plus compresser le texte, on ne peut pas proposer un codage plus court que le texte lui-m&ecirc;me).</p>
<p>L&rsquo;entropie d&rsquo;un syst&egrave;me ferm&eacute; ne peut jamais d&eacute;croitre parce que, une fois que deux gaz se sont m&eacute;lang&eacute;s par exemple, la structure initiale de l&rsquo;information a &eacute;t&eacute; d&eacute;truite (le code a disparu). On a besoin de plus de bits pour d&eacute;crire le syst&egrave;me parce que la forme compacte, mieux structur&eacute;e, s&rsquo;est volatilis&eacute;e. Le syst&egrave;me a en quelque sorte perdu la m&eacute;moire de son &eacute;tat ant&eacute;rieur. C&rsquo;est entre autre pour cette raison qu&rsquo;un &oelig;uf une fois cass&eacute; ne se reconstitue jamais. C&rsquo;est pour cela aussi qu&rsquo;on parle de fl&egrave;che du temps.</p>
<p>On voit ainsi comment la th&eacute;orie de l&rsquo;information et la thermodynamique nouent des liens qui ne sont pas uniquement d&rsquo;ordre m&eacute;taphorique (<a href="http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm">depuis 1948 les scientifiques et les philosophes ne cessent de discuter de ce lien</a>). La formulation identique de l&rsquo;entropie dans les deux domaines r&eacute;v&egrave;le une similitude d&rsquo;ordre physique mais aussi psychologique.</p>
<p>Une information que je connais d&eacute;j&agrave; ou que je ne comprends pas ne constitue pas pour moins une r&eacute;serve de n&eacute;guentropie alors qu&rsquo;elle peut l&rsquo;&ecirc;tre pour celui qui ignore l&rsquo;information et la comprend. Ainsi il reste toujours difficile de s&eacute;parer l&rsquo;information de sa signification. Le codage est du domaine de la physique, l&rsquo;information le d&eacute;passe, notamment par son aspect qualitatif.</p>
<p><a href="../../../../2009/09/10/le-flux-troisieme-etat-de-linformation/index.html">Quand je suppose que l&rsquo;information peut comme l&rsquo;eau se trouver dans plusieurs &eacute;tats</a>, je joue donc un jeu dangereux d&rsquo;un point de vue scientifique. En toute rigueur, je ne devrais donc parler que de l&rsquo;information qui se code.</p>
<p>Je persiste n&eacute;anmoins. Pour observer les diff&eacute;rents &eacute;tats de l&rsquo;eau, on fait varier la temp&eacute;rature et la pression. Qu&rsquo;est-ce que cela peut signifier pour l&rsquo;information ?</p>
<p>Lorsque la temp&eacute;rature est nulle, z&eacute;ro absolu, l&rsquo;entropie est nulle. Le syst&egrave;me est parfaitement ordonn&eacute;. La quantit&eacute; d&rsquo;information pour le d&eacute;crire est minimale. Lorsque la temp&eacute;rature s&rsquo;&eacute;l&egrave;ve, le syst&egrave;me s&rsquo;agite, il faut de plus en plus d&rsquo;information pour le d&eacute;crire. L&rsquo;entropie augmente.</p>
<p>1/T = Delta S/Delta U</p>
<p>Cette formule lie la temp&eacute;rature T &agrave; la variation de l&rsquo;entropie S et &agrave; la variation du nombre de particules dans le syst&egrave;me U. Elle signifie que plus il y a de particules, plus la temp&eacute;rature est grande et que plus la temp&eacute;rature s&rsquo;&eacute;l&egrave;ve, plus l&rsquo;entropie est grande.</p>
<p>Si on remplace U par la quantit&eacute; d&rsquo;information selon Shannon, on d&eacute;finit une forme de temp&eacute;rature pour l&rsquo;information. Elle s&rsquo;&eacute;l&egrave;ve quand le syst&egrave;me re&ccedil;oit plus d&rsquo;information. Pour un texte grav&eacute; dans le marbre, la quantit&eacute; d&rsquo;information ne varie pas. Rien ne bouge. La temp&eacute;rature est minimale. Elle s&rsquo;&eacute;l&egrave;ve quand il y a interaction. Lorsque nous parlons ou nous branchons sur les flux.</p>
<p>Delta S/ Delta V = p/T</p>
<p>Cette formule lie l&rsquo;entropie, le volume du syst&egrave;me, la pression et la temp&eacute;rature. La pression serait l&rsquo;&eacute;quivalent de la vitesse &agrave; laquelle on injecte l&rsquo;information dans le syst&egrave;me.</p>
<p>J&rsquo;avoue que je n&rsquo;ai plus l&rsquo;esprit assez matheux pour tenter de tracer un diagramme des phases pour l&rsquo;information afin de rep&eacute;rer les diff&eacute;rents &eacute;tats possibles. J&rsquo;ai effectu&eacute; quelques recherches sans rien trouver. Si un physicien passe par l&agrave;, je l&rsquo;appelle au secours. Dans mon prochain livre, la m&eacute;taphore me suffit, appuy&eacute;e sur cette vague justification. J&rsquo;ai l&rsquo;habitude de me fier &agrave; mon intuition.</p>
<p><em>PS : Ce texte sera ins&eacute;r&eacute; en hypernote dans mon prochain livre.</em></p></body></html>
<p></p><div class="sharebas pageGutter"><div class="sharemain"><div class="share" id="share11364"><div class="crunchify-social"><a class="icon_com icon" href="../../../../mail/index.html" target="_blank"></a><a class="icon_twitter icon" href="https://twitter.com/intent/tweet?text=L%E2%80%99information+dans+tous+ses+%C3%A9tats&amp;url=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F&amp;via=tcrouzet" target="_blank"></a><a class="icon_fb icon" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F" target="_blank"></a><a class="icon_linkedin icon" href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A8888%2F2009%2F10%2F21%2Fl%25e2%2580%2599information-dans-tous-ses-etats%2F&amp;title=L%E2%80%99information+dans+tous+ses+%C3%A9tats" target="_blank"></a></div></div></div></div></div></article>