<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<!DOCTYPE html><html lang=fr><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><title>L’information dans tous ses états - Thierry Crouzet</title><link rel="shortcut icon" href=/favicon.ico><link rel=alternate type=application/rss+xml title="Thierry Crouzet" href=/feeds/feed.xml><link rel=preload href="/style.css?ver=1761979510.7583401" as=style><link rel=stylesheet href="/style.css?ver=1761979510.7583401"><meta name=robots content="index, follow"><meta name=google-site-verification content=EmtKKbIk2-Tz3Nyp7hJRPjcMbgf68Vwq6yJYs1hvsqA><meta name=msvalidate.01 content=805E5BFBA891A41968028F5C97A228D3><meta name=description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta name=author content="Thierry Crouzet"><link href=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ rel=canonical><meta name=fediverse:creator content=@tcrouzet@mamot.fr><meta property=og:title content="L’information dans tous ses états"><meta property=og:description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta content=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ property=og:url><meta property=og:locale content=fr_FR><meta property=og:type content=article><meta property=og:site_name content="Thierry Crouzet"><meta property=article:published_time content=2009-10-21T12:54:00+02:00><meta property=article:modified_time content=2024-04-30T10:02:06+02:00><meta property=og:image content=https://tcrouzet.com//images_tc/2009/10/ptrible.jpeg><meta property=og:image:width content=450><meta property=og:image:height content=346><meta property=og:image:type content=image/jpeg><meta property=og:image:alt content="Point triple"><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=/images_tc/2009/10/ptrible.jpeg></head><body><header><a href=/menu/ aria-label=Menu></a><a aria-label=Accueil href=/ title=Accueil>Thierry <b>CROUZET</b></a><a href=/search/ aria-label=Recherche></a></header><main><article itemscope itemtype=https://schema.org/BlogPosting><header><figure><img width=450 height=346 src=/images_tc/2009/10/ptrible.webp class="poster-img poster-img-full" alt="Point triple" fetchpriority=high decoding=async srcset="/images_tc/2009/10/ptrible-250.webp 250w,/images_tc/2009/10/ptrible.webp 450w" sizes="(max-width: 450px) 100vw, 450px"><figcaption>Point triple</figcaption></figure><h1 itemprop=headline>L’information dans tous ses états</h1></header><section itemprop=articleBody><p>Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un message (en écartant le bruit causé par les parasites par exemple). Pour lui, l’information mesure ce que nous apprenons (si nous avons déjà reçu le message, nous n’apprenons rien).</p><p>En 1948, Claude Shannon prolongea cette idée et publia sa théorie de l’information (ou pour être précis sa théorie de la communication). Pour lui, la quantité d’information contenue dans un message équivaut au nombre de bits pour l’encoder.</p><p>Si le message indique soit blanc, soit noir, un seul bit est nécessaire. La mesure de l’information selon Shannon est indépendante de sa signification. Un listing peut contenir beaucoup d’informations qui ont peu de signification, un aphorisme peut contenir peu d’information et avoir une signification profonde. La signification existe dans l’esprit de l’émetteur et du récepteur, pas dans l’information qui circule.</p><p>Von Neumann n’appréciait pas l’idée de séparer information et signification. Il remarqua que la formule de Shannon pour décrire l’information était identique à celle pour d’écrire l’entropie (au signe moins près). Il aurait donc mieux valu parler d’entropie (ou de néguentropie) mais Shannon continua à parler d’information parce que les ingénieurs employaient déjà ce mot.</p><p>Mais qu’est-ce que l’entropie ? Elle indique le niveau de désordre dans un système (alors que la néguentropie indique son niveau d’ordre). Le second principe de la thermodynamique stipule que l’entropie d’un système fermé augmente toujours jusqu’à atteindre son maximal.</p><p>Exemple le plus simple. On place deux gaz différents dans deux réservoirs. Les particules sont relativement ordonnées, les unes dans un réservoir, les autres dans l’autre. Si on connecte les deux réservoirs, les particules de l’un passent dans l’autre jusqu’à ce que nous obtenions un mélange parfait, c’est-à-dire un désordre maximal.</p><p>Mais qu’est-ce que le désordre maximal ? Les physiciens ont découvert que c’est l’état où le système enferme le plus d’information possible. Exemple. Dans un livre, les lettres sont relativement ordonnées, formant des mots qui se répètent de temps à autre. Ce texte peut-être compressé : on remplace chaque mot qui se répète par un code plus court. En revanche, si on mélange les lettres au hasard, on augmente le désordre, perd la signification mais la quantité d’information nécessaire à décrire le système augmente (on ne peut plus compresser le texte, on ne peut pas proposer un codage plus court que le texte lui-même).</p><p>L’entropie d’un système fermé ne peut jamais décroitre parce que, une fois que deux gaz se sont mélangés par exemple, la structure initiale de l’information a été détruite (le code a disparu). On a besoin de plus de bits pour décrire le système parce que la forme compacte, mieux structurée, s’est volatilisée. Le système a en quelque sorte perdu la mémoire de son état antérieur. C’est entre autre pour cette raison qu’un œuf une fois cassé ne se reconstitue jamais. C’est pour cela aussi qu’on parle de flèche du temps.</p><p>On voit ainsi comment la théorie de l’information et la thermodynamique nouent des liens qui ne sont pas uniquement d’ordre métaphorique (<a href=http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm rel="noopener noreferrer">depuis 1948 les scientifiques et les philosophes ne cessent de discuter de ce lien</a>). La formulation identique de l’entropie dans les deux domaines révèle une similitude d’ordre physique mais aussi psychologique.</p><p>Une information que je connais déjà ou que je ne comprends pas ne constitue pas pour moins une réserve de néguentropie alors qu’elle peut l’être pour celui qui ignore l’information et la comprend. Ainsi il reste toujours difficile de séparer l’information de sa signification. Le codage est du domaine de la physique, l’information le dépasse, notamment par son aspect qualitatif.</p><p><a href=/2009/09/10/le-flux-troisieme-etat-de-linformation/ >Quand je suppose que l’information peut comme l’eau se trouver dans plusieurs états</a>, je joue donc un jeu dangereux d’un point de vue scientifique. En toute rigueur, je ne devrais donc parler que de l’information qui se code.</p><p>Je persiste néanmoins. Pour observer les différents états de l’eau, on fait varier la température et la pression. Qu’est-ce que cela peut signifier pour l’information ?</p><p>Lorsque la température est nulle, zéro absolu, l’entropie est nulle. Le système est parfaitement ordonné. La quantité d’information pour le décrire est minimale. Lorsque la température s’élève, le système s’agite, il faut de plus en plus d’information pour le décrire. L’entropie augmente.</p><p>1/T = Delta S/Delta U</p><p>Cette formule lie la température T à la variation de l’entropie S et à la variation du nombre de particules dans le système U. Elle signifie que plus il y a de particules, plus la température est grande et que plus la température s’élève, plus l’entropie est grande.</p><p>Si on remplace U par la quantité d’information selon Shannon, on définit une forme de température pour l’information. Elle s’élève quand le système reçoit plus d’information. Pour un texte gravé dans le marbre, la quantité d’information ne varie pas. Rien ne bouge. La température est minimale. Elle s’élève quand il y a interaction. Lorsque nous parlons ou nous branchons sur les flux.</p><p>Delta S/ Delta V = p/T</p><p>Cette formule lie l’entropie, le volume du système, la pression et la température. La pression serait l’équivalent de la vitesse à laquelle on injecte l’information dans le système.</p><p>J’avoue que je n’ai plus l’esprit assez matheux pour tenter de tracer un diagramme des phases pour l’information afin de repérer les différents états possibles. J’ai effectué quelques recherches sans rien trouver. Si un physicien passe par là, je l’appelle au secours. Dans mon prochain livre, la métaphore me suffit, appuyée sur cette vague justification. J’ai l’habitude de me fier à mon intuition.</p><p><em>PS : Ce texte sera inséré en hypernote dans mon prochain livre.</em></p></section><footer><nav><a href=/2009/10/19/dans-la-peau-de-finkielkraut/ class=prev rel=prev>&lt; </a><a href=/tag/netculture itemprop=articleSection>NetCulture 23</a> ● <a href=/2009/ ><time datetime=2009-10-21T12:54:00 title="Publié à 12:54">21 octobre 2009</time></a><a href=/2009/10/22/long-tail-mon-cul/ class=next rel=next> &gt;</a></nav><section><a class=icon_download href=https://raw.githubusercontent.com/tcrouzet/md/refs/heads/main/2009/10/linformation-dans-tous-ses-etats.md download title=Télécharger aria-label="Télécharger en Markdown"></a><a class=icon_friends href=/page/lectures/ title="Sites amis" aria-label="Afficher les sites amis"></a></section><div><a href=/page/abonnement-par-mail/ ><svg width=24 height=18 viewbox="0 0 24 18" fill=none xmlns=http://www.w3.org/2000/svg><path d="M21.75 1.5H2.25c-.828 0-1.5.672-1.5 1.5v12c0 .828.672 1.5 1.5 1.5h19.5c.828 0 1.5-.672 1.5-1.5V3c0-.828-.672-1.5-1.5-1.5zM15.687 6.975L19.5 10.5M8.313 6.975L4.5 10.5" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path><path d="M22.88 2.014l-9.513 6.56C12.965 8.851 12.488 9 12 9s-.965-.149-1.367-.426L1.12 2.014" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path></svg> Newsletters </a><a href=/page/me-soutenir/ class=link-background> Me soutenir </a></div></footer></article></main></body></html>
=======
<!DOCTYPE html><html lang=fr><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><title>L’information dans tous ses états - Thierry Crouzet</title><link rel="shortcut icon" href=/favicon.ico><link rel=alternate type=application/rss+xml title="Thierry Crouzet" href=/feeds/feed.xml><!-- Google Fonts --><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Bitter:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel=stylesheet><link rel=preload href="/style.css?ver=52" as=style><link rel=stylesheet href="/style.css?ver=52"><meta name=robots content="index, follow"><meta name=google-site-verification content=EmtKKbIk2-Tz3Nyp7hJRPjcMbgf68Vwq6yJYs1hvsqA><meta name=msvalidate.01 content=805E5BFBA891A41968028F5C97A228D3><meta name=description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta name=author content="Thierry Crouzet"><link href=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ rel=canonical><meta name=fediverse:creator content=@tcrouzet@mamot.fr><meta property=og:title content="L’information dans tous ses états"><meta property=og:description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta content=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ property=og:url><meta property=og:locale content=fr_FR><meta property=og:type content=article><meta property=og:site_name content="Thierry Crouzet"><meta property=article:published_time content=2009-10-21T12:54:00+02:00><meta property=article:modified_time content=2024-04-30T10:02:06+02:00><meta property=og:image content=https://tcrouzet.com//images_tc/2009/10/ptrible.jpeg><meta property=og:image:width content=450><meta property=og:image:height content=346><meta property=og:image:type content=image/jpeg><meta property=og:image:alt content="Point triple"><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=/images_tc/2009/10/ptrible.jpeg></head><body><header><a href=/menu/ aria-label=Menu></a><a aria-label=Accueil href=/ title=Accueil>Thierry <b>CROUZET</b></a><a href=/search/ aria-label=Recherche></a></header><main><article itemscope itemtype=https://schema.org/BlogPosting><header><figure><img width=450 height=346 src=/images_tc/2009/10/ptrible.webp class="poster-img poster-img-full" alt="Point triple" fetchpriority=high decoding=async srcset="/images_tc/2009/10/ptrible.webp 250w,/images_tc/2009/10/ptrible.webp 450w" sizes="(max-width: 450px) 100vw, 450px"><figcaption>Point triple</figcaption></figure><h1 itemprop=headline>L’information dans tous ses états</h1><nav><a href=/2009/10/19/dans-la-peau-de-finkielkraut/ class=prev rel=prev>&lt; </a><a href=/tag/netculture itemprop=articleSection>NetCulture 23</a> ● <a href=/2009/ ><time datetime=2009-10-21T12:54:00 title="Publié à 12:54">21 octobre 2009</time></a><a href=/2009/10/22/long-tail-mon-cul/ class=next rel=next> &gt;</a></nav></header><section itemprop=articleBody><p>Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un message (en écartant le bruit causé par les parasites par exemple). Pour lui, l’information mesure ce que nous apprenons (si nous avons déjà reçu le message, nous n’apprenons rien).</p><p>En 1948, Claude Shannon prolongea cette idée et publia sa théorie de l’information (ou pour être précis sa théorie de la communication). Pour lui, la quantité d’information contenue dans un message équivaut au nombre de bits pour l’encoder.</p><p>Si le message indique soit blanc, soit noir, un seul bit est nécessaire. La mesure de l’information selon Shannon est indépendante de sa signification. Un listing peut contenir beaucoup d’informations qui ont peu de signification, un aphorisme peut contenir peu d’information et avoir une signification profonde. La signification existe dans l’esprit de l’émetteur et du récepteur, pas dans l’information qui circule.</p><p>Von Neumann n’appréciait pas l’idée de séparer information et signification. Il remarqua que la formule de Shannon pour décrire l’information était identique à celle pour d’écrire l’entropie (au signe moins près). Il aurait donc mieux valu parler d’entropie (ou de néguentropie) mais Shannon continua à parler d’information parce que les ingénieurs employaient déjà ce mot.</p><p>Mais qu’est-ce que l’entropie ? Elle indique le niveau de désordre dans un système (alors que la néguentropie indique son niveau d’ordre). Le second principe de la thermodynamique stipule que l’entropie d’un système fermé augmente toujours jusqu’à atteindre son maximal.</p><p>Exemple le plus simple. On place deux gaz différents dans deux réservoirs. Les particules sont relativement ordonnées, les unes dans un réservoir, les autres dans l’autre. Si on connecte les deux réservoirs, les particules de l’un passent dans l’autre jusqu’à ce que nous obtenions un mélange parfait, c’est-à-dire un désordre maximal.</p><p>Mais qu’est-ce que le désordre maximal ? Les physiciens ont découvert que c’est l’état où le système enferme le plus d’information possible. Exemple. Dans un livre, les lettres sont relativement ordonnées, formant des mots qui se répètent de temps à autre. Ce texte peut-être compressé : on remplace chaque mot qui se répète par un code plus court. En revanche, si on mélange les lettres au hasard, on augmente le désordre, perd la signification mais la quantité d’information nécessaire à décrire le système augmente (on ne peut plus compresser le texte, on ne peut pas proposer un codage plus court que le texte lui-même).</p><p>L’entropie d’un système fermé ne peut jamais décroitre parce que, une fois que deux gaz se sont mélangés par exemple, la structure initiale de l’information a été détruite (le code a disparu). On a besoin de plus de bits pour décrire le système parce que la forme compacte, mieux structurée, s’est volatilisée. Le système a en quelque sorte perdu la mémoire de son état antérieur. C’est entre autre pour cette raison qu’un œuf une fois cassé ne se reconstitue jamais. C’est pour cela aussi qu’on parle de flèche du temps.</p><p>On voit ainsi comment la théorie de l’information et la thermodynamique nouent des liens qui ne sont pas uniquement d’ordre métaphorique (<a href=http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm rel="noopener noreferrer">depuis 1948 les scientifiques et les philosophes ne cessent de discuter de ce lien</a>). La formulation identique de l’entropie dans les deux domaines révèle une similitude d’ordre physique mais aussi psychologique.</p><p>Une information que je connais déjà ou que je ne comprends pas ne constitue pas pour moins une réserve de néguentropie alors qu’elle peut l’être pour celui qui ignore l’information et la comprend. Ainsi il reste toujours difficile de séparer l’information de sa signification. Le codage est du domaine de la physique, l’information le dépasse, notamment par son aspect qualitatif.</p><p><a href=/2009/09/10/le-flux-troisieme-etat-de-linformation/ >Quand je suppose que l’information peut comme l’eau se trouver dans plusieurs états</a>, je joue donc un jeu dangereux d’un point de vue scientifique. En toute rigueur, je ne devrais donc parler que de l’information qui se code.</p><p>Je persiste néanmoins. Pour observer les différents états de l’eau, on fait varier la température et la pression. Qu’est-ce que cela peut signifier pour l’information ?</p><p>Lorsque la température est nulle, zéro absolu, l’entropie est nulle. Le système est parfaitement ordonné. La quantité d’information pour le décrire est minimale. Lorsque la température s’élève, le système s’agite, il faut de plus en plus d’information pour le décrire. L’entropie augmente.</p><p>1/T = Delta S/Delta U</p><p>Cette formule lie la température T à la variation de l’entropie S et à la variation du nombre de particules dans le système U. Elle signifie que plus il y a de particules, plus la température est grande et que plus la température s’élève, plus l’entropie est grande.</p><p>Si on remplace U par la quantité d’information selon Shannon, on définit une forme de température pour l’information. Elle s’élève quand le système reçoit plus d’information. Pour un texte gravé dans le marbre, la quantité d’information ne varie pas. Rien ne bouge. La température est minimale. Elle s’élève quand il y a interaction. Lorsque nous parlons ou nous branchons sur les flux.</p><p>Delta S/ Delta V = p/T</p><p>Cette formule lie l’entropie, le volume du système, la pression et la température. La pression serait l’équivalent de la vitesse à laquelle on injecte l’information dans le système.</p><p>J’avoue que je n’ai plus l’esprit assez matheux pour tenter de tracer un diagramme des phases pour l’information afin de repérer les différents états possibles. J’ai effectué quelques recherches sans rien trouver. Si un physicien passe par là, je l’appelle au secours. Dans mon prochain livre, la métaphore me suffit, appuyée sur cette vague justification. J’ai l’habitude de me fier à mon intuition.</p><p><em>PS : Ce texte sera inséré en hypernote dans mon prochain livre.</em></p></section><footer><section><button class=icon_com onclick="showComments('com1710')" title=Commenter aria-label="Afficher les commentaires"></button><button class=icon_share onclick=copyText() title=Partager aria-label="Partager l'article"></button><a class=icon_download href=https://raw.githubusercontent.com/tcrouzet/md/refs/heads/main/2009/10/linformation-dans-tous-ses-etats.md download title=Télécharger aria-label="Télécharger en Markdown"></a><a class=icon_friends href=/page/lectures/ title="Sites amis" aria-label="Afficher les sites amis"></a></section><div class=comments id=com1710 data-post-url=2009/10/21/linformation-dans-tous-ses-etats/ data-post-title="L’information dans tous ses états" data-post-date=2009-10-21T12:54:00+02:00 hidden></div><div><a href=/page/abonnement-par-mail/ ><svg width=24 height=18 viewbox="0 0 24 18" fill=none xmlns=http://www.w3.org/2000/svg><path d="M21.75 1.5H2.25c-.828 0-1.5.672-1.5 1.5v12c0 .828.672 1.5 1.5 1.5h19.5c.828 0 1.5-.672 1.5-1.5V3c0-.828-.672-1.5-1.5-1.5zM15.687 6.975L19.5 10.5M8.313 6.975L4.5 10.5" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path><path d="M22.88 2.014l-9.513 6.56C12.965 8.851 12.488 9 12 9s-.965-.149-1.367-.426L1.12 2.014" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path></svg> Newsletters </a><a href=/page/me-soutenir/ class=link-background> Me soutenir </a></div></footer></article><div id=loadMore next-url=/2009/10/19/dans-la-peau-de-finkielkraut/ hidden></div></main><script src="/toggle.js?ver=52" defer></script></body></html>
>>>>>>> Stashed changes
=======
<!DOCTYPE html><html lang=fr><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><title>L’information dans tous ses états - Thierry Crouzet</title><link rel="shortcut icon" href=/favicon.ico><link rel=alternate type=application/rss+xml title="Thierry Crouzet" href=/feeds/feed.xml><!-- Google Fonts --><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Bitter:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel=stylesheet><link rel=preload href="/style.css?ver=52" as=style><link rel=stylesheet href="/style.css?ver=52"><meta name=robots content="index, follow"><meta name=google-site-verification content=EmtKKbIk2-Tz3Nyp7hJRPjcMbgf68Vwq6yJYs1hvsqA><meta name=msvalidate.01 content=805E5BFBA891A41968028F5C97A228D3><meta name=description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta name=author content="Thierry Crouzet"><link href=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ rel=canonical><meta name=fediverse:creator content=@tcrouzet@mamot.fr><meta property=og:title content="L’information dans tous ses états"><meta property=og:description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta content=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ property=og:url><meta property=og:locale content=fr_FR><meta property=og:type content=article><meta property=og:site_name content="Thierry Crouzet"><meta property=article:published_time content=2009-10-21T12:54:00+02:00><meta property=article:modified_time content=2024-04-30T10:02:06+02:00><meta property=og:image content=https://tcrouzet.com//images_tc/2009/10/ptrible.jpeg><meta property=og:image:width content=450><meta property=og:image:height content=346><meta property=og:image:type content=image/jpeg><meta property=og:image:alt content="Point triple"><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=/images_tc/2009/10/ptrible.jpeg></head><body><header><a href=/menu/ aria-label=Menu></a><a aria-label=Accueil href=/ title=Accueil>Thierry <b>CROUZET</b></a><a href=/search/ aria-label=Recherche></a></header><main><article itemscope itemtype=https://schema.org/BlogPosting><header><figure><img width=450 height=346 src=/images_tc/2009/10/ptrible.webp class="poster-img poster-img-full" alt="Point triple" fetchpriority=high decoding=async srcset="/images_tc/2009/10/ptrible.webp 250w,/images_tc/2009/10/ptrible.webp 450w" sizes="(max-width: 450px) 100vw, 450px"><figcaption>Point triple</figcaption></figure><h1 itemprop=headline>L’information dans tous ses états</h1><nav><a href=/2009/10/19/dans-la-peau-de-finkielkraut/ class=prev rel=prev>&lt; </a><a href=/tag/netculture itemprop=articleSection>NetCulture 23</a> ● <a href=/2009/ ><time datetime=2009-10-21T12:54:00 title="Publié à 12:54">21 octobre 2009</time></a><a href=/2009/10/22/long-tail-mon-cul/ class=next rel=next> &gt;</a></nav></header><section itemprop=articleBody><p>Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un message (en écartant le bruit causé par les parasites par exemple). Pour lui, l’information mesure ce que nous apprenons (si nous avons déjà reçu le message, nous n’apprenons rien).</p><p>En 1948, Claude Shannon prolongea cette idée et publia sa théorie de l’information (ou pour être précis sa théorie de la communication). Pour lui, la quantité d’information contenue dans un message équivaut au nombre de bits pour l’encoder.</p><p>Si le message indique soit blanc, soit noir, un seul bit est nécessaire. La mesure de l’information selon Shannon est indépendante de sa signification. Un listing peut contenir beaucoup d’informations qui ont peu de signification, un aphorisme peut contenir peu d’information et avoir une signification profonde. La signification existe dans l’esprit de l’émetteur et du récepteur, pas dans l’information qui circule.</p><p>Von Neumann n’appréciait pas l’idée de séparer information et signification. Il remarqua que la formule de Shannon pour décrire l’information était identique à celle pour d’écrire l’entropie (au signe moins près). Il aurait donc mieux valu parler d’entropie (ou de néguentropie) mais Shannon continua à parler d’information parce que les ingénieurs employaient déjà ce mot.</p><p>Mais qu’est-ce que l’entropie ? Elle indique le niveau de désordre dans un système (alors que la néguentropie indique son niveau d’ordre). Le second principe de la thermodynamique stipule que l’entropie d’un système fermé augmente toujours jusqu’à atteindre son maximal.</p><p>Exemple le plus simple. On place deux gaz différents dans deux réservoirs. Les particules sont relativement ordonnées, les unes dans un réservoir, les autres dans l’autre. Si on connecte les deux réservoirs, les particules de l’un passent dans l’autre jusqu’à ce que nous obtenions un mélange parfait, c’est-à-dire un désordre maximal.</p><p>Mais qu’est-ce que le désordre maximal ? Les physiciens ont découvert que c’est l’état où le système enferme le plus d’information possible. Exemple. Dans un livre, les lettres sont relativement ordonnées, formant des mots qui se répètent de temps à autre. Ce texte peut-être compressé : on remplace chaque mot qui se répète par un code plus court. En revanche, si on mélange les lettres au hasard, on augmente le désordre, perd la signification mais la quantité d’information nécessaire à décrire le système augmente (on ne peut plus compresser le texte, on ne peut pas proposer un codage plus court que le texte lui-même).</p><p>L’entropie d’un système fermé ne peut jamais décroitre parce que, une fois que deux gaz se sont mélangés par exemple, la structure initiale de l’information a été détruite (le code a disparu). On a besoin de plus de bits pour décrire le système parce que la forme compacte, mieux structurée, s’est volatilisée. Le système a en quelque sorte perdu la mémoire de son état antérieur. C’est entre autre pour cette raison qu’un œuf une fois cassé ne se reconstitue jamais. C’est pour cela aussi qu’on parle de flèche du temps.</p><p>On voit ainsi comment la théorie de l’information et la thermodynamique nouent des liens qui ne sont pas uniquement d’ordre métaphorique (<a href=http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm rel="noopener noreferrer">depuis 1948 les scientifiques et les philosophes ne cessent de discuter de ce lien</a>). La formulation identique de l’entropie dans les deux domaines révèle une similitude d’ordre physique mais aussi psychologique.</p><p>Une information que je connais déjà ou que je ne comprends pas ne constitue pas pour moins une réserve de néguentropie alors qu’elle peut l’être pour celui qui ignore l’information et la comprend. Ainsi il reste toujours difficile de séparer l’information de sa signification. Le codage est du domaine de la physique, l’information le dépasse, notamment par son aspect qualitatif.</p><p><a href=/2009/09/10/le-flux-troisieme-etat-de-linformation/ >Quand je suppose que l’information peut comme l’eau se trouver dans plusieurs états</a>, je joue donc un jeu dangereux d’un point de vue scientifique. En toute rigueur, je ne devrais donc parler que de l’information qui se code.</p><p>Je persiste néanmoins. Pour observer les différents états de l’eau, on fait varier la température et la pression. Qu’est-ce que cela peut signifier pour l’information ?</p><p>Lorsque la température est nulle, zéro absolu, l’entropie est nulle. Le système est parfaitement ordonné. La quantité d’information pour le décrire est minimale. Lorsque la température s’élève, le système s’agite, il faut de plus en plus d’information pour le décrire. L’entropie augmente.</p><p>1/T = Delta S/Delta U</p><p>Cette formule lie la température T à la variation de l’entropie S et à la variation du nombre de particules dans le système U. Elle signifie que plus il y a de particules, plus la température est grande et que plus la température s’élève, plus l’entropie est grande.</p><p>Si on remplace U par la quantité d’information selon Shannon, on définit une forme de température pour l’information. Elle s’élève quand le système reçoit plus d’information. Pour un texte gravé dans le marbre, la quantité d’information ne varie pas. Rien ne bouge. La température est minimale. Elle s’élève quand il y a interaction. Lorsque nous parlons ou nous branchons sur les flux.</p><p>Delta S/ Delta V = p/T</p><p>Cette formule lie l’entropie, le volume du système, la pression et la température. La pression serait l’équivalent de la vitesse à laquelle on injecte l’information dans le système.</p><p>J’avoue que je n’ai plus l’esprit assez matheux pour tenter de tracer un diagramme des phases pour l’information afin de repérer les différents états possibles. J’ai effectué quelques recherches sans rien trouver. Si un physicien passe par là, je l’appelle au secours. Dans mon prochain livre, la métaphore me suffit, appuyée sur cette vague justification. J’ai l’habitude de me fier à mon intuition.</p><p><em>PS : Ce texte sera inséré en hypernote dans mon prochain livre.</em></p></section><footer><section><button class=icon_com onclick="showComments('com1710')" title=Commenter aria-label="Afficher les commentaires"></button><button class=icon_share onclick=copyText() title=Partager aria-label="Partager l'article"></button><a class=icon_download href=https://raw.githubusercontent.com/tcrouzet/md/refs/heads/main/2009/10/linformation-dans-tous-ses-etats.md download title=Télécharger aria-label="Télécharger en Markdown"></a><a class=icon_friends href=/page/lectures/ title="Sites amis" aria-label="Afficher les sites amis"></a></section><div class=comments id=com1710 data-post-url=2009/10/21/linformation-dans-tous-ses-etats/ data-post-title="L’information dans tous ses états" data-post-date=2009-10-21T12:54:00+02:00 hidden></div><div><a href=/page/abonnement-par-mail/ ><svg width=24 height=18 viewbox="0 0 24 18" fill=none xmlns=http://www.w3.org/2000/svg><path d="M21.75 1.5H2.25c-.828 0-1.5.672-1.5 1.5v12c0 .828.672 1.5 1.5 1.5h19.5c.828 0 1.5-.672 1.5-1.5V3c0-.828-.672-1.5-1.5-1.5zM15.687 6.975L19.5 10.5M8.313 6.975L4.5 10.5" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path><path d="M22.88 2.014l-9.513 6.56C12.965 8.851 12.488 9 12 9s-.965-.149-1.367-.426L1.12 2.014" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path></svg> Newsletters </a><a href=/page/me-soutenir/ class=link-background> Me soutenir </a></div></footer></article><div id=loadMore next-url=/2009/10/19/dans-la-peau-de-finkielkraut/ hidden></div></main><script src="/toggle.js?ver=52" defer></script></body></html>
>>>>>>> Stashed changes
=======
<!DOCTYPE html><html lang=fr><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><title>L’information dans tous ses états - Thierry Crouzet</title><link rel="shortcut icon" href=/favicon.ico><link rel=alternate type=application/rss+xml title="Thierry Crouzet" href=/feeds/feed.xml><!-- Google Fonts --><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&family=Bitter:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel=stylesheet><link rel=preload href="/style.css?ver=52" as=style><link rel=stylesheet href="/style.css?ver=52"><meta name=robots content="index, follow"><meta name=google-site-verification content=EmtKKbIk2-Tz3Nyp7hJRPjcMbgf68Vwq6yJYs1hvsqA><meta name=msvalidate.01 content=805E5BFBA891A41968028F5C97A228D3><meta name=description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta name=author content="Thierry Crouzet"><link href=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ rel=canonical><meta name=fediverse:creator content=@tcrouzet@mamot.fr><meta property=og:title content="L’information dans tous ses états"><meta property=og:description content="Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un mess…"><meta content=https://tcrouzet.com/2009/10/21/linformation-dans-tous-ses-etats/ property=og:url><meta property=og:locale content=fr_FR><meta property=og:type content=article><meta property=og:site_name content="Thierry Crouzet"><meta property=article:published_time content=2009-10-21T12:54:00+02:00><meta property=article:modified_time content=2024-04-30T10:02:06+02:00><meta property=og:image content=https://tcrouzet.com//images_tc/2009/10/ptrible.jpeg><meta property=og:image:width content=450><meta property=og:image:height content=346><meta property=og:image:type content=image/jpeg><meta property=og:image:alt content="Point triple"><meta name=twitter:card content=summary_large_image><meta name=twitter:image content=/images_tc/2009/10/ptrible.jpeg></head><body><header><a href=/menu/ aria-label=Menu></a><a aria-label=Accueil href=/ title=Accueil>Thierry <b>CROUZET</b></a><a href=/search/ aria-label=Recherche></a></header><main><article itemscope itemtype=https://schema.org/BlogPosting><header><figure><img width=450 height=346 src=/images_tc/2009/10/ptrible.webp class="poster-img poster-img-full" alt="Point triple" fetchpriority=high decoding=async srcset="/images_tc/2009/10/ptrible.webp 250w,/images_tc/2009/10/ptrible.webp 450w" sizes="(max-width: 450px) 100vw, 450px"><figcaption>Point triple</figcaption></figure><h1 itemprop=headline>L’information dans tous ses états</h1><nav><a href=/2009/10/19/dans-la-peau-de-finkielkraut/ class=prev rel=prev>&lt; </a><a href=/tag/netculture itemprop=articleSection>NetCulture 23</a> ● <a href=/2009/ ><time datetime=2009-10-21T12:54:00 title="Publié à 12:54">21 octobre 2009</time></a><a href=/2009/10/22/long-tail-mon-cul/ class=next rel=next> &gt;</a></nav></header><section itemprop=articleBody><p>Le vocable information a été employé pour la première fois comme nouvelle unité par Ralph Hartley des Bell Labs en 1928 pour désigner la partie utile d’un message (en écartant le bruit causé par les parasites par exemple). Pour lui, l’information mesure ce que nous apprenons (si nous avons déjà reçu le message, nous n’apprenons rien).</p><p>En 1948, Claude Shannon prolongea cette idée et publia sa théorie de l’information (ou pour être précis sa théorie de la communication). Pour lui, la quantité d’information contenue dans un message équivaut au nombre de bits pour l’encoder.</p><p>Si le message indique soit blanc, soit noir, un seul bit est nécessaire. La mesure de l’information selon Shannon est indépendante de sa signification. Un listing peut contenir beaucoup d’informations qui ont peu de signification, un aphorisme peut contenir peu d’information et avoir une signification profonde. La signification existe dans l’esprit de l’émetteur et du récepteur, pas dans l’information qui circule.</p><p>Von Neumann n’appréciait pas l’idée de séparer information et signification. Il remarqua que la formule de Shannon pour décrire l’information était identique à celle pour d’écrire l’entropie (au signe moins près). Il aurait donc mieux valu parler d’entropie (ou de néguentropie) mais Shannon continua à parler d’information parce que les ingénieurs employaient déjà ce mot.</p><p>Mais qu’est-ce que l’entropie ? Elle indique le niveau de désordre dans un système (alors que la néguentropie indique son niveau d’ordre). Le second principe de la thermodynamique stipule que l’entropie d’un système fermé augmente toujours jusqu’à atteindre son maximal.</p><p>Exemple le plus simple. On place deux gaz différents dans deux réservoirs. Les particules sont relativement ordonnées, les unes dans un réservoir, les autres dans l’autre. Si on connecte les deux réservoirs, les particules de l’un passent dans l’autre jusqu’à ce que nous obtenions un mélange parfait, c’est-à-dire un désordre maximal.</p><p>Mais qu’est-ce que le désordre maximal ? Les physiciens ont découvert que c’est l’état où le système enferme le plus d’information possible. Exemple. Dans un livre, les lettres sont relativement ordonnées, formant des mots qui se répètent de temps à autre. Ce texte peut-être compressé : on remplace chaque mot qui se répète par un code plus court. En revanche, si on mélange les lettres au hasard, on augmente le désordre, perd la signification mais la quantité d’information nécessaire à décrire le système augmente (on ne peut plus compresser le texte, on ne peut pas proposer un codage plus court que le texte lui-même).</p><p>L’entropie d’un système fermé ne peut jamais décroitre parce que, une fois que deux gaz se sont mélangés par exemple, la structure initiale de l’information a été détruite (le code a disparu). On a besoin de plus de bits pour décrire le système parce que la forme compacte, mieux structurée, s’est volatilisée. Le système a en quelque sorte perdu la mémoire de son état antérieur. C’est entre autre pour cette raison qu’un œuf une fois cassé ne se reconstitue jamais. C’est pour cela aussi qu’on parle de flèche du temps.</p><p>On voit ainsi comment la théorie de l’information et la thermodynamique nouent des liens qui ne sont pas uniquement d’ordre métaphorique (<a href=http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm rel="noopener noreferrer">depuis 1948 les scientifiques et les philosophes ne cessent de discuter de ce lien</a>). La formulation identique de l’entropie dans les deux domaines révèle une similitude d’ordre physique mais aussi psychologique.</p><p>Une information que je connais déjà ou que je ne comprends pas ne constitue pas pour moins une réserve de néguentropie alors qu’elle peut l’être pour celui qui ignore l’information et la comprend. Ainsi il reste toujours difficile de séparer l’information de sa signification. Le codage est du domaine de la physique, l’information le dépasse, notamment par son aspect qualitatif.</p><p><a href=/2009/09/10/le-flux-troisieme-etat-de-linformation/ >Quand je suppose que l’information peut comme l’eau se trouver dans plusieurs états</a>, je joue donc un jeu dangereux d’un point de vue scientifique. En toute rigueur, je ne devrais donc parler que de l’information qui se code.</p><p>Je persiste néanmoins. Pour observer les différents états de l’eau, on fait varier la température et la pression. Qu’est-ce que cela peut signifier pour l’information ?</p><p>Lorsque la température est nulle, zéro absolu, l’entropie est nulle. Le système est parfaitement ordonné. La quantité d’information pour le décrire est minimale. Lorsque la température s’élève, le système s’agite, il faut de plus en plus d’information pour le décrire. L’entropie augmente.</p><p>1/T = Delta S/Delta U</p><p>Cette formule lie la température T à la variation de l’entropie S et à la variation du nombre de particules dans le système U. Elle signifie que plus il y a de particules, plus la température est grande et que plus la température s’élève, plus l’entropie est grande.</p><p>Si on remplace U par la quantité d’information selon Shannon, on définit une forme de température pour l’information. Elle s’élève quand le système reçoit plus d’information. Pour un texte gravé dans le marbre, la quantité d’information ne varie pas. Rien ne bouge. La température est minimale. Elle s’élève quand il y a interaction. Lorsque nous parlons ou nous branchons sur les flux.</p><p>Delta S/ Delta V = p/T</p><p>Cette formule lie l’entropie, le volume du système, la pression et la température. La pression serait l’équivalent de la vitesse à laquelle on injecte l’information dans le système.</p><p>J’avoue que je n’ai plus l’esprit assez matheux pour tenter de tracer un diagramme des phases pour l’information afin de repérer les différents états possibles. J’ai effectué quelques recherches sans rien trouver. Si un physicien passe par là, je l’appelle au secours. Dans mon prochain livre, la métaphore me suffit, appuyée sur cette vague justification. J’ai l’habitude de me fier à mon intuition.</p><p><em>PS : Ce texte sera inséré en hypernote dans mon prochain livre.</em></p></section><footer><section><button class=icon_com onclick="showComments('com1710')" title=Commenter aria-label="Afficher les commentaires"></button><button class=icon_share onclick=copyText() title=Partager aria-label="Partager l'article"></button><a class=icon_download href=https://raw.githubusercontent.com/tcrouzet/md/refs/heads/main/2009/10/linformation-dans-tous-ses-etats.md download title=Télécharger aria-label="Télécharger en Markdown"></a><a class=icon_friends href=/page/lectures/ title="Sites amis" aria-label="Afficher les sites amis"></a></section><div class=comments id=com1710 data-post-url=2009/10/21/linformation-dans-tous-ses-etats/ data-post-title="L’information dans tous ses états" data-post-date=2009-10-21T12:54:00+02:00 hidden></div><div><a href=/page/abonnement-par-mail/ ><svg width=24 height=18 viewbox="0 0 24 18" fill=none xmlns=http://www.w3.org/2000/svg><path d="M21.75 1.5H2.25c-.828 0-1.5.672-1.5 1.5v12c0 .828.672 1.5 1.5 1.5h19.5c.828 0 1.5-.672 1.5-1.5V3c0-.828-.672-1.5-1.5-1.5zM15.687 6.975L19.5 10.5M8.313 6.975L4.5 10.5" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path><path d="M22.88 2.014l-9.513 6.56C12.965 8.851 12.488 9 12 9s-.965-.149-1.367-.426L1.12 2.014" stroke=#fff stroke-width=1.5 stroke-linecap=round stroke-linejoin=round></path></svg> Newsletters </a><a href=/page/me-soutenir/ class=link-background> Me soutenir </a></div></footer></article><div id=loadMore next-url=/2009/10/19/dans-la-peau-de-finkielkraut/ hidden></div></main><script src="/toggle.js?ver=52" defer></script></body></html>
>>>>>>> Stashed changes
