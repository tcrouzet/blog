<!DOCTYPE>
		<html lang="fr" xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://opengraphprotocol.org/schema/" xmlns:fb="http://www.facebook.com/2008/fbml">
		<head profile="http://gmpg.org/xfn/11">
		  <title>L’information dans tous ses états</title>
		  <meta charset="utf-8">
		  <meta http-equiv="Cache-control" content="public">
		  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		  <meta name="viewport" content="width=device-width, initial-scale=1.0">
		  <meta name="apple-mobile-web-app-capable" content="yes">
		  <meta name="apple-mobile-web-app-status-bar-style" content="black">
		  <link href="../../assets/minimal_static.css" rel="stylesheet" type="text/css" media="screen" >
		  <link href="../../assets/minimal_static.css" rel="stylesheet" type="text/css" media="print" >
		  </head><body><article id="wrapper">

<h1>L’information dans tous ses états</h1>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<?xml version="1.0" encoding="UTF-8"?><html><body><div class="image"><img src="../../images/11364_ptrible.jpg" alt="Point triple" width="450" height="346"><div class="legend">Point triple</div></div>
<p>Le vocable information a &eacute;t&eacute; employ&eacute; pour la premi&egrave;re fois comme
nouvelle unit&eacute; par Ralph Hartley des Bell Labs en 1928 pour
d&eacute;signer la partie utile d&rsquo;un message (en &eacute;cartant le bruit caus&eacute;
par les parasites par exemple). Pour lui, l&rsquo;information mesure ce
que nous apprenons (si nous avons d&eacute;j&agrave; re&ccedil;u le message, nous
n&rsquo;apprenons rien).<!--more--></p>
<p>En 1948, Claude Shannon prolongea cette id&eacute;e et publia sa
th&eacute;orie de l&rsquo;information (ou pour &ecirc;tre pr&eacute;cis sa th&eacute;orie de la
communication). Pour lui, la quantit&eacute; d&rsquo;information contenue dans
un message &eacute;quivaut au nombre de bits pour l&rsquo;encoder.</p>
<p>Si le message indique soit blanc, soit noir, un seul bit est
n&eacute;cessaire. La mesure de l&rsquo;information selon Shannon est
ind&eacute;pendante de sa signification. Un listing peut contenir beaucoup
d&rsquo;informations qui ont peu de signification, un aphorisme peut
contenir peu d&rsquo;information et avoir une signification profonde. La
signification existe dans l&rsquo;esprit de l&rsquo;&eacute;metteur et du r&eacute;cepteur,
pas dans l&rsquo;information qui circule.</p>
<p>Von Neumann n&rsquo;appr&eacute;ciait pas l&rsquo;id&eacute;e de s&eacute;parer information et
signification. Il remarqua que la formule de Shannon pour d&eacute;crire
l&rsquo;information &eacute;tait identique &agrave; celle pour d&rsquo;&eacute;crire l&rsquo;entropie (au
signe moins pr&egrave;s). Il aurait donc mieux valu parler d&rsquo;entropie (ou
de n&eacute;guentropie) mais Shannon continua &agrave; parler d&rsquo;information parce
que les ing&eacute;nieurs employaient d&eacute;j&agrave; ce mot.</p>
<p>Mais qu&rsquo;est-ce que l&rsquo;entropie ? Elle indique le niveau de
d&eacute;sordre dans un syst&egrave;me (alors que la n&eacute;guentropie indique son
niveau d&rsquo;ordre). Le second principe de la thermodynamique stipule
que l&rsquo;entropie d&rsquo;un syst&egrave;me ferm&eacute; augmente toujours jusqu&rsquo;&agrave;
atteindre son maximal.</p>
<p>Exemple le plus simple. On place deux gaz diff&eacute;rents dans deux
r&eacute;servoirs. Les particules sont relativement ordonn&eacute;es, les unes
dans un r&eacute;servoir, les autres dans l&rsquo;autre. Si on connecte les deux
r&eacute;servoirs, les particules de l&rsquo;un passent dans l&rsquo;autre jusqu&rsquo;&agrave; ce
que nous obtenions un m&eacute;lange parfait, c&rsquo;est-&agrave;-dire un d&eacute;sordre
maximal.</p>
<p>Mais qu&rsquo;est-ce que le d&eacute;sordre maximal ? Les physiciens ont
d&eacute;couvert que c&rsquo;est l&rsquo;&eacute;tat o&ugrave; le syst&egrave;me enferme le plus
d&rsquo;information possible. Exemple. Dans un livre, les lettres sont
relativement ordonn&eacute;es, formant des mots qui se r&eacute;p&egrave;tent de temps &agrave;
autre. Ce texte peut-&ecirc;tre compress&eacute; : on remplace chaque mot qui se
r&eacute;p&egrave;te par un code plus court. En revanche, si on m&eacute;lange les
lettres au hasard, on augmente le d&eacute;sordre, perd la signification
mais la quantit&eacute; d&rsquo;information n&eacute;cessaire &agrave; d&eacute;crire le syst&egrave;me
augmente (on ne peut plus compresser le texte, on ne peut pas
proposer un codage plus court que le texte lui-m&ecirc;me).</p>
<p>L&rsquo;entropie d&rsquo;un syst&egrave;me ferm&eacute; ne peut jamais d&eacute;croitre parce
que, une fois que deux gaz se sont m&eacute;lang&eacute;s par exemple, la
structure initiale de l&rsquo;information a &eacute;t&eacute; d&eacute;truite (le code a
disparu). On a besoin de plus de bits pour d&eacute;crire le syst&egrave;me parce
que la forme compacte, mieux structur&eacute;e, s&rsquo;est volatilis&eacute;e. Le
syst&egrave;me a en quelque sorte perdu la m&eacute;moire de son &eacute;tat ant&eacute;rieur.
C&rsquo;est entre autre pour cette raison qu&rsquo;un &oelig;uf une fois cass&eacute; ne se
reconstitue jamais. C&rsquo;est pour cela aussi qu&rsquo;on parle de fl&egrave;che du
temps.</p>
<p>On voit ainsi comment la th&eacute;orie de l&rsquo;information et la
thermodynamique nouent des liens qui ne sont pas uniquement d&rsquo;ordre
m&eacute;taphorique (<a href="http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5btxt.htm">depuis
1948 les scientifiques et les philosophes ne cessent de discuter de
ce lien</a>). La formulation identique de l&rsquo;entropie dans les deux
domaines r&eacute;v&egrave;le une similitude d&rsquo;ordre physique mais aussi
psychologique.</p>
<p>Une information que je connais d&eacute;j&agrave; ou que je ne comprends pas
ne constitue pas pour moins une r&eacute;serve de n&eacute;guentropie alors
qu&rsquo;elle peut l&rsquo;&ecirc;tre pour celui qui ignore l&rsquo;information et la
comprend. Ainsi il reste toujours difficile de s&eacute;parer
l&rsquo;information de sa signification. Le codage est du domaine de la
physique, l&rsquo;information le d&eacute;passe, notamment par son aspect
qualitatif.</p>
<p><a href="http://blog.tcrouzet.com/2009/09/10/le-flux-troisieme-etat-de-linformation/">
Quand je suppose que l&rsquo;information peut comme l&rsquo;eau se trouver dans
plusieurs &eacute;tats</a>, je joue donc un jeu dangereux d&rsquo;un point de
vue scientifique. En toute rigueur, je ne devrais donc parler que
de l&rsquo;information qui se code.</p>
<p>Je persiste n&eacute;anmoins. Pour observer les diff&eacute;rents &eacute;tats de
l&rsquo;eau, on fait varier la temp&eacute;rature et la pression. Qu&rsquo;est-ce que
cela peut signifier pour l&rsquo;information ?</p>
<p>Lorsque la temp&eacute;rature est nulle, z&eacute;ro absolu, l&rsquo;entropie est
nulle. Le syst&egrave;me est parfaitement ordonn&eacute;. La quantit&eacute;
d&rsquo;information pour le d&eacute;crire est minimale. Lorsque la temp&eacute;rature
s&rsquo;&eacute;l&egrave;ve, le syst&egrave;me s&rsquo;agite, il faut de plus en plus d&rsquo;information
pour le d&eacute;crire. L&rsquo;entropie augmente.</p>
<p>1/T = Delta S/Delta U</p>
<p>Cette formule lie la temp&eacute;rature T &agrave; la variation de l&rsquo;entropie
S et &agrave; la variation du nombre de particules dans le syst&egrave;me U. Elle
signifie que plus il y a de particules, plus la temp&eacute;rature est
grande et que plus la temp&eacute;rature s&rsquo;&eacute;l&egrave;ve, plus l&rsquo;entropie est
grande.</p>
<p>Si on remplace U par la quantit&eacute; d&rsquo;information selon Shannon, on
d&eacute;finit une forme de temp&eacute;rature pour l&rsquo;information. Elle s&rsquo;&eacute;l&egrave;ve
quand le syst&egrave;me re&ccedil;oit plus d&rsquo;information. Pour un texte grav&eacute;
dans le marbre, la quantit&eacute; d&rsquo;information ne varie pas. Rien ne
bouge. La temp&eacute;rature est minimale. Elle s&rsquo;&eacute;l&egrave;ve quand il y a
interaction. Lorsque nous parlons ou nous branchons sur les
flux.</p>
<p>Delta S/ Delta V = p/T</p>
<p>Cette formule lie l&rsquo;entropie, le volume du syst&egrave;me, la pression
et la temp&eacute;rature. La pression serait l&rsquo;&eacute;quivalent de la vitesse &agrave;
laquelle on injecte l&rsquo;information dans le syst&egrave;me.</p>
<p>J&rsquo;avoue que je n&rsquo;ai plus l&rsquo;esprit assez matheux pour tenter de
tracer un diagramme des phases pour l&rsquo;information afin de rep&eacute;rer
les diff&eacute;rents &eacute;tats possibles. J&rsquo;ai effectu&eacute; quelques recherches
sans rien trouver. Si un physicien passe par l&agrave;, je l&rsquo;appelle au
secours. Dans mon prochain livre, la m&eacute;taphore me suffit, appuy&eacute;e
sur cette vague justification. J&rsquo;ai l&rsquo;habitude de me fier &agrave; mon
intuition.</p>
<p><em>PS : Ce texte sera ins&eacute;r&eacute; en hypernote dans mon prochain
livre.</em></p>
</body></html>

<p class="date"><br/><a href="../2009/11422.html">Suite</a> | <a href="../2009/index.html">2009</a> | <a href="../index.html">Sommaire</a> | Texte publié mercredi 21 octobre 2009</p>


</article></body></html>